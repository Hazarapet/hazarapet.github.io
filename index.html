<!DOCTYPE html>
<html>

  <head>
    
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  Hazarapet  Tunanyan


</title>
<meta name="description" content="Personal page of Hazarapet Tunanyan.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<!-- <link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" /> -->
<link rel="stylesheet" href="https://raw.githubusercontent.com/jwarby/pygments-css/master/github.css" />

<!-- Styles -->

<link rel="icon" href="/assets/img/favicon.png">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->






    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav sticky-bottom-footer">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
        <!-- Social Icons -->
        <div class="navbar-brand social">
          <a href="mailto:%68%61%7A%61%72%61%70%65%74%74%75%6E%61%6E%79%61%6E@%67%6D%61%69%6C.%63%6F%6D"><i class="fas fa-envelope"></i></a>

<a href="https://scholar.google.com/citations?user=FG7MnFcAAAAJ" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="fas fa-graduation-cap"></i></a>


<a href="https://github.com/hazarapet" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a>
<a href="https://www.linkedin.com/in/hazarapet" title="LinkedIn" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin"></i></a>
<a href="https://twitter.com/hazarapett" title="Twitter" target="_blank" rel="noopener noreferrer"><i class="fab fa-twitter"></i></a>











        </div>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item active">
            <a class="nav-link" href="/">
              about
              
                <span class="sr-only">(current)</span>
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                publications
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/teaching/">
                teaching
                
              </a>
          </li>
          
          
          
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">
     <span class="font-weight-bold">Hazarapet</span>  Tunanyan
    </h1>
     <p class="desc">Staff Data Scientist at Grid Dynamics.</p>
  </header>

  <article>
    
    <div class="profile float-right">
      
        <img class="img-fluid z-depth-1 rounded" src="/assets/img/avatar3.png">
      
      
    </div>
    

    <div class="clearfix">
      <p>I am a Staff Data Scientist at Grid Dynamics working on computer vision and deep learning problems. 
Previously I have been a Senior Machine Learning Scientist at Picsart, working on generative tasks such as text-to-image synthisis and its customizations. 
Also, I worked on other computer vision tasks which included semantic and instance segmentation, object detection, and object tracking.</p>

<p>My research interests align with Deep Generative Methods, Conditional and Unconditional Diffusion, Multi-Modal Large Language Models, and Iterative Optimization Problems.</p>

<p>I hold M.Sc. and B.Sc. degrees in Informatics and Applied Mathematics
from <strong><a href="http://www.ysu.am/ysu/en" target="_blank" rel="noopener noreferrer">Yerevan State University (YSU)</a></strong> 
and have more than 5 years of experience in software engineering.</p>

    </div>

    
      <div class="news">
  <h2>news</h2>
  
    <div class="table-responsive">
      <table class="table table-sm table-borderless">
      
      
        <tr>
          <th scope="row">Aug 6, 2024</th>
          <td>
            
              Joined Grid Dynamics as a Staff Data Scientist.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Sep 9, 2023</th>
          <td>
            
              Gave a talk at the <strong><a href="https://datafest.am/2023/speakers" target="_blank" rel="noopener noreferrer">Datafest 2023</a></strong> conference.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Mar 1, 2023</th>
          <td>
            
              A paper has been accepted at <strong><a href="https://cvpr2023.thecvf.com/" target="_blank" rel="noopener noreferrer">CVPR2023</a></strong> conference.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Oct 19, 2021</th>
          <td>
            
              A paper has been accepted at <strong><a href="https://www.bmvc2021.com/" target="_blank" rel="noopener noreferrer">BMVC2021</a></strong> conference.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Sep 1, 2019</th>
          <td>
            
              Joined <strong><a href="http://www.ysu.am/ysu/en" target="_blank" rel="noopener noreferrer">Yerevan State University (YSU)</a></strong> as a Adjunct Lecturer.

            
          </td>
        </tr>
      
      </table>
    </div>
  
</div>

    

    
      <div class="publications">
  <h2>publications</h2>
  <ol class="bibliography">
<li>
<div class="row">
  <div class="col-sm-3 abbr">
  
    <div class="cover">
      
      <img src="/assets/bibliography/img/multi-concept-t2i-zero.png" alt="paper cover" width="200">
      
    </div>
  
  
    
    <abbr class="badge">Preprint</abbr>
    
  
  </div>

  <div id="multi-concept-t2i-zero" class="col-sm-7">
    
      <div class="title">Multi-Concept T2I-Zero: Tweaking Only The Text Embeddings and Nothing Else</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Hazarapet Tunanyan</em>,
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://ir1d.github.io/" target="_blank" rel="noopener noreferrer">Dejia Xu</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://www.linkedin.com/in/shant-navasardyan-1302aa149/" target="_blank" rel="noopener noreferrer">Shant Navasardyan</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://www.ece.utexas.edu/people/faculty/atlas-wang" target="_blank" rel="noopener noreferrer">Zhangyang Wang</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="https://www.humphreyshi.com/" target="_blank" rel="noopener noreferrer">Humphrey Shi</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Arxiv preprint</em>
      
      
        2023
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2310.07419" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
    
    
    
    
    
    
    
    
    
    
      <a href="https://multi-concept-t2i-zero.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Recent advances in text-to-image diffusion models have enabled the photorealistic generation of images from text prompts. 
    Despite the great progress, existing models still struggle to generate compositional multi-concept images naturally, 
    limiting their ability to visualize human imagination. While several recent works have attempted to address this issue, 
    they either introduce additional training or adopt guidance at inference time. In this work, we consider a more ambitious goal: 
    natural multi-concept generation using a pre-trained diffusion model, and with almost no extra cost. To achieve this goal, 
    we identify the limitations in the text embeddings used for the pre-trained text-to-image diffusion models. Specifically, 
    we observe concept dominance and non-localized contribution that severely degrade multi-concept generation performance. 
    We further design a minimal low-cost solution that overcomes the above issues by tweaking (not re-training) the text embeddings 
    for more realistic multi-concept text-to-image generation. Our Correction by Similarities method tweaks the embedding of 
    concepts by collecting semantic features from most similar tokens to localize the contribution. To avoid mixing features of concepts, 
    we also apply Cross-Token Non-Maximum Suppression, which excludes the overlap of contributions from different concepts. 
    Experiments show that our approach outperforms previous methods in text-to-image, image manipulation, and personalization tasks, 
    despite not introducing additional training or inference costs to the diffusion steps.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-3 abbr">
  
    <div class="cover">
      
      <img src="/assets/bibliography/img/specialist-diffusion.png" alt="paper cover" width="200">
      
    </div>
  
  
    
    <abbr class="badge">CVPR</abbr>
    
  
  </div>

  <div id="specialist-diffusion" class="col-sm-7">
    
      <div class="title">Specialist Diffusion: Plug-and-Play Sample-Efficient Fine-Tuning of Text-to-Image Diffusion Models to Learn Any Unseen Style</div>
      <div class="author">
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://github.com/alonewithyou" target="_blank" rel="noopener noreferrer">Haoming Lu</a>,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Hazarapet Tunanyan</em>,
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://wangk.ai/" target="_blank" rel="noopener noreferrer">Kai Wang</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://www.linkedin.com/in/shant-navasardyan-1302aa149/" target="_blank" rel="noopener noreferrer">Shant Navasardyan</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  <a href="https://www.ece.utexas.edu/people/faculty/atlas-wang" target="_blank" rel="noopener noreferrer">Zhangyang Wang</a>,
                
              
            
          
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                
                  and <a href="https://www.humphreyshi.com/" target="_blank" rel="noopener noreferrer">Humphrey Shi</a>
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In IEEE / CVF Computer Vision and Pattern Recognition Conference (CVPR)</em>
      
      
        2023
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      
      <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lu_Specialist_Diffusion_Plug-and-Play_Sample-Efficient_Fine-Tuning_of_Text-to-Image_Diffusion_Models_To_CVPR_2023_paper.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a>
      
    
    
    
    
      <a href="https://github.com/Picsart-AI-Research/Specialist-Diffusion" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
      
      <a href="/assets/bibliography/poster/specialist-diffusion-poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a>
      
    
    
    
      <a href="https://specialist-diffusion.github.io" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Diffusion models have demonstrated impressive capability of text-conditioned image synthesis, and broader application horizons 
    are emerging by personalizing those pretrained diffusion models toward generating some specialized target object or style. 
    In this paper, we aim to learn an unseen style by simply fine-tuning a pre-trained diffusion model with a handful of images 
    (e.g., less than 10), so that the fine-tuned model can generate high-quality images of arbitrary objects in this style. 
    Such extremely lowshot fine-tuning is accomplished by a novel toolkit of finetuning techniques, including text-to-image 
    customized data augmentations, a content loss to facilitate content-style disentanglement, and sparse updating that 
    focuses on only a few time steps. Our framework, dubbed Specialist Diffusion, is plug-and-play to existing diffusion model 
    backbones and other personalization techniques. We demonstrate it to outperform the latest few-shot personalization 
    alternatives of diffusion models such as Textual Inversion and DreamBooth, in terms of learning highly sophisticated 
    styles with ultra-sample-efficient tuning. We further show that Specialist Diffusion can be integrated on top of 
    Textual Inversion to boost performance further, even on highly unusual styles.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-3 abbr">
  
    <div class="cover">
      
      <img src="/assets/bibliography/img/c4net.png" alt="paper cover" width="200">
      
    </div>
  
  
    
    <abbr class="badge">BMVC</abbr>
    
  
  </div>

  <div id="c4net" class="col-sm-7">
    
      <div class="title">C4Net: Contextual Compression and Complementary Combination Network for Salient Object Detection</div>
      <div class="author">
        
          
          
          
          
          
          
            
              <em>Hazarapet Tunanyan</em>
            
          
        
      </div>

      <div class="periodical">
      
        <em>In British Machine Vision Conference (BMVC)</em>
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2110.11887" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
    
    
    
    
    
    
    
    
      
      <a href="https://www.bmvc2021-virtualconference.com/conference/papers/paper_1077.html" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Poster</a>
      
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Deep learning solutions of the salient object detection problem have achieved great results in recent years.
    The majority of these models are based on encoders and decoders, with a different multi-feature combination.
    In this paper, we show that feature concatenation works better than other combination methods like multiplication or
    addition. Also, joint feature learning gives better results, because of the information sharing during their processing.
    We designed a Complementary Extraction Module (CEM) to extract necessary features with edge preservation.
    Our proposed Excessiveness Loss (EL) function helps to reduce false-positive predictions and purifies the edges with other
    weighted loss functions. Our designed Pyramid-Semantic Module (PSM) with Global guiding flow (G) makes the
    prediction more accurate by providing high-level complementary information to shallower layers.
    Experimental results show that the proposed model outperforms the state-of-theart
    methods on all benchmark datasets under three evaluation metrics</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
</ol>
</div>

    

    
  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="sticky-bottom mt-5">
  <div class="container">
    Â© Copyright 2024 Hazarapet  Tunanyan.
    
    
    
  </div>
</footer>



  </body>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


</html>
