---
---

@string{BMVC = {British Machine Vision Conference (BMVC)}}
@string{CVPR = {IEEE / CVF Computer Vision and Pattern Recognition Conference (CVPR)}}

@inproceedings{multi-concept-t2i-zero,
  abbr={Arxiv},
  title={Multi-Concept T2I-Zero: Tweaking Only The Text Embeddings and Nothing Else},
  author={Tunanyan, Hazarapet and Xu, Dejia and Navasardyan, Shant and Wang, Zhangyang and Shi, Humphrey},
  abstract={Recent advances in text-to-image diffusion models have enabled the photorealistic generation of images from text prompts. 
    Despite the great progress, existing models still struggle to generate compositional multi-concept images naturally, 
    limiting their ability to visualize human imagination. While several recent works have attempted to address this issue, 
    they either introduce additional training or adopt guidance at inference time. In this work, we consider a more ambitious goal: 
    natural multi-concept generation using a pre-trained diffusion model, and with almost no extra cost. To achieve this goal, 
    we identify the limitations in the text embeddings used for the pre-trained text-to-image diffusion models. Specifically, 
    we observe concept dominance and non-localized contribution that severely degrade multi-concept generation performance. 
    We further design a minimal low-cost solution that overcomes the above issues by tweaking (not re-training) the text embeddings 
    for more realistic multi-concept text-to-image generation. Our Correction by Similarities method tweaks the embedding of 
    concepts by collecting semantic features from most similar tokens to localize the contribution. To avoid mixing features of concepts, 
    we also apply Cross-Token Non-Maximum Suppression, which excludes the overlap of contributions from different concepts. 
    Experiments show that our approach outperforms previous methods in text-to-image, image manipulation, and personalization tasks, 
    despite not introducing additional training or inference costs to the diffusion steps.},
  year={2023},
  website={https://multi-concept-t2i-zero.github.io/},
  booktitle={Arxiv preprint},
  arxiv={2310.07419}
  cover={multi-concept-t2i-zero.png},
  selected={true}
}

@inproceedings{specialist-diffusion,
  abbr={CVPR},
  title={Specialist Diffusion: Plug-and-Play Sample-Efficient Fine-Tuning of Text-to-Image Diffusion Models to Learn Any Unseen Style},
  author={Lu, Haoming and Tunanyan, Hazarapet and Wang, Kai and Navasardyan, Shant and Wang, Zhangyang and Shi, Humphrey},
  abstract={Diffusion models have demonstrated impressive capability of text-conditioned image synthesis, and broader application horizons 
    are emerging by personalizing those pretrained diffusion models toward generating some specialized target object or style. 
    In this paper, we aim to learn an unseen style by simply fine-tuning a pre-trained diffusion model with a handful of images 
    (e.g., less than 10), so that the fine-tuned model can generate high-quality images of arbitrary objects in this style. 
    Such extremely lowshot fine-tuning is accomplished by a novel toolkit of finetuning techniques, including text-to-image 
    customized data augmentations, a content loss to facilitate content-style disentanglement, and sparse updating that 
    focuses on only a few time steps. Our framework, dubbed Specialist Diffusion, is plug-and-play to existing diffusion model 
    backbones and other personalization techniques. We demonstrate it to outperform the latest few-shot personalization 
    alternatives of diffusion models such as Textual Inversion and DreamBooth, in terms of learning highly sophisticated 
    styles with ultra-sample-efficient tuning. We further show that Specialist Diffusion can be integrated on top of 
    Textual Inversion to boost performance further, even on highly unusual styles.},
  year={2023},
  month={Jun},
  website={https://specialist-diffusion.github.io},
  code={https://github.com/Picsart-AI-Research/Specialist-Diffusion},
  booktitle=CVPR,
  poster={specialist-diffusion-poster.pdf},
  paper={https://openaccess.thecvf.com/content/CVPR2023/papers/Lu_Specialist_Diffusion_Plug-and-Play_Sample-Efficient_Fine-Tuning_of_Text-to-Image_Diffusion_Models_To_CVPR_2023_paper.pdf},
  cover={specialist-diffusion.png},
  selected={true}
}

@inproceedings{c4net,
  abbr={BMVC},
  title={C4Net: Contextual Compression and Complementary Combination Network for Salient Object Detection},
  author={Tunanyan, Hazarapet},
  abstract={Deep learning solutions of the salient object detection problem have achieved great results in recent years.
    The majority of these models are based on encoders and decoders, with a different multi-feature combination.
    In this paper, we show that feature concatenation works better than other combination methods like multiplication or
    addition. Also, joint feature learning gives better results, because of the information sharing during their processing.
    We designed a Complementary Extraction Module (CEM) to extract necessary features with edge preservation.
    Our proposed Excessiveness Loss (EL) function helps to reduce false-positive predictions and purifies the edges with other
    weighted loss functions. Our designed Pyramid-Semantic Module (PSM) with Global guiding flow (G) makes the
    prediction more accurate by providing high-level complementary information to shallower layers.
    Experimental results show that the proposed model outperforms the state-of-theart
    methods on all benchmark datasets under three evaluation metrics},
  year={2021},
  month={Nov},
  booktitle=BMVC,
  arxiv={2110.11887},
  poster={https://www.bmvc2021-virtualconference.com/conference/papers/paper_1077.html},
  cover={c4net.png},
  selected={true}
}

